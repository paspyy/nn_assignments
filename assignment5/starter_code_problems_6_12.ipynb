{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modular network implementation\n",
    "\n",
    "In the following cells, I implement in a modular way a feedforward neural network. Please study the code -- many network implementations follow a similar pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# These are taken from https://github.com/mila-udem/blocks\n",
    "# \n",
    "\n",
    "class Constant():\n",
    "    \"\"\"Initialize parameters to a constant.\n",
    "    The constant may be a scalar or a :class:`~numpy.ndarray` of any shape\n",
    "    that is broadcastable with the requested parameter arrays.\n",
    "    Parameters\n",
    "    ----------\n",
    "    constant : :class:`~numpy.ndarray`\n",
    "        The initialization value to use. Must be a scalar or an ndarray (or\n",
    "        compatible object, such as a nested list) that has a shape that is\n",
    "        broadcastable with any shape requested by `initialize`.\n",
    "    \"\"\"\n",
    "    def __init__(self, constant):\n",
    "        self._constant = numpy.asarray(constant)\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        dest = numpy.empty(shape, dtype=np.float32)\n",
    "        dest[...] = self._constant\n",
    "        return dest\n",
    "\n",
    "\n",
    "class IsotropicGaussian():\n",
    "    \"\"\"Initialize parameters from an isotropic Gaussian distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    std : float, optional\n",
    "        The standard deviation of the Gaussian distribution. Defaults to 1.\n",
    "    mean : float, optional\n",
    "        The mean of the Gaussian distribution. Defaults to 0\n",
    "    Notes\n",
    "    -----\n",
    "    Be careful: the standard deviation goes first and the mean goes\n",
    "    second!\n",
    "    \"\"\"\n",
    "    def __init__(self, std=1, mean=0):\n",
    "        self._mean = mean\n",
    "        self._std = std\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        m = rng.normal(self._mean, self._std, size=shape)\n",
    "        return m.astype(np.float32)\n",
    "\n",
    "\n",
    "class Uniform():\n",
    "    \"\"\"Initialize parameters from a uniform distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    mean : float, optional\n",
    "        The mean of the uniform distribution (i.e. the center of mass for\n",
    "        the density function); Defaults to 0.\n",
    "    width : float, optional\n",
    "        One way of specifying the range of the uniform distribution. The\n",
    "        support will be [mean - width/2, mean + width/2]. **Exactly one**\n",
    "        of `width` or `std` must be specified.\n",
    "    std : float, optional\n",
    "        An alternative method of specifying the range of the uniform\n",
    "        distribution. Chooses the width of the uniform such that random\n",
    "        variates will have a desired standard deviation. **Exactly one** of\n",
    "        `width` or `std` must be specified.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean=0., width=None, std=None):\n",
    "        if (width is not None) == (std is not None):\n",
    "            raise ValueError(\"must specify width or std, \"\n",
    "                             \"but not both\")\n",
    "        if std is not None:\n",
    "            # Variance of a uniform is 1/12 * width^2\n",
    "            self._width = numpy.sqrt(12) * std\n",
    "        else:\n",
    "            self._width = width\n",
    "        self._mean = mean\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        w = self._width / 2\n",
    "        m = rng.uniform(self._mean - w, self._mean + w, size=shape)\n",
    "        return m.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self, rng=None):\n",
    "        if rng is None:\n",
    "            rng = numpy.random\n",
    "        self.rng = rng\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        return []\n",
    "    \n",
    "    def get_gradients(self, dLdY, fprop_context):\n",
    "        return []\n",
    "    \n",
    "\n",
    "class AffineLayer(Layer):\n",
    "    def __init__(self, num_in, num_out, weight_init=None, bias_init=None, **kwargs):\n",
    "        super(AffineLayer, self).__init__(**kwargs)\n",
    "        if weight_init is None:\n",
    "            \n",
    "            # TODO propose a default initialization scheme.\n",
    "            # Type a sentence explaining why, and if you use a reference, \n",
    "            # cite it here\n",
    "            weight_init = Uniform(width=2.0/sqrt(num_in))\n",
    "        if bias_init is None:\n",
    "            bias_init = Constant(0.0)\n",
    "        \n",
    "        self.W = weight_init.generate(self.rng, (num_out, num_in))\n",
    "        self.b = bias_init.generate(self.rng, (num_out, 1))\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return [self.W, self.b]\n",
    "    \n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        return ['W','b']\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        #Save X for later reusal\n",
    "        fprop_context = dict(X=X)\n",
    "        Y = np.dot(self.W, X) +  self.b\n",
    "        return Y, fprop_context\n",
    "    \n",
    "    def bprop(self, dLdY, fprop_context):\n",
    "        \n",
    "        # TODO: fill in gradient computation\n",
    "        dLdX = self.W.T.dot(dLdY)\n",
    "        \n",
    "        return dLdX\n",
    "    \n",
    "    def get_gradients(self, dLdY, fprop_context):\n",
    "        X = fprop_context['X']\n",
    "        dLdW = np.dot(dLdY, X.T)\n",
    "        dLdb = dLdY.sum(1, keepdims=True)\n",
    "        return [dLdW, dLdb]\n",
    "    \n",
    "class TanhLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(TanhLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        Y = np.tanh(X)\n",
    "        fprop_context = dict(Y=Y)\n",
    "        return Y, fprop_context\n",
    "    \n",
    "    def bprop(self, dLdY, fprop_context):\n",
    "        Y = fprop_context['Y']\n",
    "        \n",
    "        # Fill in proper gradient computation\n",
    "        dLdX = (1-Y**2)*dLdY\n",
    "        \n",
    "        return dLdX\n",
    "\n",
    "    \n",
    "class ReLULayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ReLULayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        Y = np.maximum(X, 0.0)\n",
    "        fprop_context = dict(Y=Y)\n",
    "        return Y, fprop_context\n",
    "    \n",
    "    def bprop(self, dLdY, fprop_context):\n",
    "        Y = fprop_context['Y']\n",
    "        return dLdY * (Y>0)\n",
    "\n",
    "    \n",
    "class SoftMaxLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SoftMaxLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def compute_probabilities(self, X):\n",
    "        O = X - X.max(axis=0, keepdims=True)\n",
    "        O = np.exp(O)\n",
    "        O /= O.sum(axis=0, keepdims=True)\n",
    "        return O\n",
    "    \n",
    "    def fprop_cost(self, X, Y):\n",
    "        NS = X.shape[1]\n",
    "        O = self.compute_probabilities(X)\n",
    "        Cost = -1.0/NS * np.log(O[Y.ravel(), range(NS)]).sum()\n",
    "        return Cost, O, dict(O=O, X=X, Y=Y)\n",
    "    \n",
    "    def bprop_cost(self, fprop_context):\n",
    "        X = fprop_context['X']\n",
    "        Y = fprop_context['Y']\n",
    "        O = fprop_context['O']\n",
    "        NS = X.shape[1]\n",
    "        dLdX = O.copy()\n",
    "        dLdX[Y, range(NS)] -= 1.0\n",
    "        dLdX /= NS\n",
    "        return dLdX\n",
    "    \n",
    "class FeedForwardNet(object):\n",
    "    def __init__(self, layers=None):\n",
    "        if layers is None:\n",
    "            layers = []\n",
    "        self.layers = layers\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params += layer.parameters\n",
    "        return params\n",
    "    \n",
    "    @parameters.setter\n",
    "    def parameters(self, values):\n",
    "        for ownP, newP in zip(self.parameters, values):\n",
    "            ownP[...] = newP\n",
    "    \n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        param_names = []\n",
    "        for layer in self.layers:\n",
    "            param_names += layer.parameter_names\n",
    "        return param_names\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        for layer in self.layers[:-1]:\n",
    "            X, fp_context = layer.fprop(X)\n",
    "        return self.layers[-1].compute_probabilities(X)\n",
    "    \n",
    "    def get_cost_and_gradient(self, X, Y):\n",
    "        fp_contexts = []\n",
    "        for layer in self.layers[:-1]:\n",
    "            X, fp_context = layer.fprop(X)\n",
    "            fp_contexts.append(fp_context)\n",
    "        \n",
    "        L, O, fp_context = self.layers[-1].fprop_cost(X, Y)\n",
    "        dLdX = self.layers[-1].bprop_cost(fp_context)\n",
    "        \n",
    "        dLdP = [] #gradient with respect to parameters\n",
    "        for i in xrange(len(self.layers)-1):\n",
    "            layer = self.layers[len(self.layers)-2-i]\n",
    "            fp_context = fp_contexts[len(self.layers)-2-i]\n",
    "            dLdP = layer.get_gradients(dLdX, fp_context) + dLdP\n",
    "            dLdX = layer.bprop(dLdX, fp_context)\n",
    "        return L, O, dLdP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#training algorithms. They change the network!\n",
    "def GD(net, X, Y, alpha=1e-4, max_iters=1000000, tolerance=1e-6):\n",
    "    \"\"\"\n",
    "    Simple batch gradient descent\n",
    "    \"\"\"\n",
    "    old_L = np.inf\n",
    "    for i in xrange(max_iters):\n",
    "        L, O, gradients = net.get_cost_and_gradient(X, Y)\n",
    "        if old_L < L:\n",
    "            print \"Iter: %d, loss increased!!\" % (i,)\n",
    "        if (old_L - L)<tolerance:\n",
    "            print \"Tolerance level reached exiting\"\n",
    "            break\n",
    "        if i % 1000 == 0:\n",
    "            err_rate = (O.argmax(0) != Y).mean()\n",
    "            print \"At iteration %d, loss %f, train error rate %f%%\" % (i, L, err_rate*100)\n",
    "        for P,G in zip(net.parameters, gradients):\n",
    "            P -= alpha * G\n",
    "        old_L = L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "IrisX = iris.data.T\n",
    "IrisX = (IrisX - IrisX.mean(axis=1, keepdims=True)) / IrisX.std(axis=1, keepdims=True)\n",
    "IrisY = iris.target.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 0, loss 1.214752, train error rate 89.333333%\n",
      "At iteration 1000, loss 0.052105, train error rate 2.000000%\n",
      "At iteration 2000, loss 0.043275, train error rate 1.333333%\n",
      "At iteration 3000, loss 0.040857, train error rate 1.333333%\n",
      "At iteration 4000, loss 0.039741, train error rate 1.333333%\n",
      "At iteration 5000, loss 0.039046, train error rate 1.333333%\n",
      "At iteration 6000, loss 0.038520, train error rate 1.333333%\n",
      "At iteration 7000, loss 0.038074, train error rate 1.333333%\n",
      "At iteration 8000, loss 0.037669, train error rate 1.333333%\n",
      "At iteration 9000, loss 0.037282, train error rate 1.333333%\n",
      "At iteration 10000, loss 0.036889, train error rate 1.333333%\n",
      "At iteration 11000, loss 0.036451, train error rate 1.333333%\n",
      "At iteration 12000, loss 0.035902, train error rate 1.333333%\n",
      "At iteration 13000, loss 0.035114, train error rate 1.333333%\n",
      "At iteration 14000, loss 0.033911, train error rate 1.333333%\n",
      "At iteration 15000, loss 0.032161, train error rate 1.333333%\n",
      "At iteration 16000, loss 0.029849, train error rate 1.333333%\n",
      "At iteration 17000, loss 0.027125, train error rate 1.333333%\n",
      "At iteration 18000, loss 0.024264, train error rate 0.666667%\n",
      "At iteration 19000, loss 0.021524, train error rate 0.666667%\n",
      "At iteration 20000, loss 0.019043, train error rate 0.666667%\n",
      "At iteration 21000, loss 0.016843, train error rate 0.666667%\n",
      "At iteration 22000, loss 0.014903, train error rate 0.666667%\n",
      "At iteration 23000, loss 0.013237, train error rate 0.666667%\n",
      "At iteration 24000, loss 0.011842, train error rate 0.000000%\n",
      "At iteration 25000, loss 0.010677, train error rate 0.000000%\n",
      "At iteration 26000, loss 0.009695, train error rate 0.000000%\n",
      "At iteration 27000, loss 0.008859, train error rate 0.000000%\n",
      "At iteration 28000, loss 0.008140, train error rate 0.000000%\n",
      "At iteration 29000, loss 0.007516, train error rate 0.000000%\n",
      "At iteration 30000, loss 0.006971, train error rate 0.000000%\n",
      "At iteration 31000, loss 0.006491, train error rate 0.000000%\n",
      "At iteration 32000, loss 0.006066, train error rate 0.000000%\n",
      "At iteration 33000, loss 0.005688, train error rate 0.000000%\n",
      "At iteration 34000, loss 0.005350, train error rate 0.000000%\n",
      "At iteration 35000, loss 0.005046, train error rate 0.000000%\n",
      "At iteration 36000, loss 0.004771, train error rate 0.000000%\n",
      "At iteration 37000, loss 0.004522, train error rate 0.000000%\n",
      "At iteration 38000, loss 0.004295, train error rate 0.000000%\n",
      "At iteration 39000, loss 0.004087, train error rate 0.000000%\n",
      "At iteration 40000, loss 0.003897, train error rate 0.000000%\n",
      "At iteration 41000, loss 0.003722, train error rate 0.000000%\n",
      "At iteration 42000, loss 0.003561, train error rate 0.000000%\n",
      "At iteration 43000, loss 0.003412, train error rate 0.000000%\n",
      "At iteration 44000, loss 0.003274, train error rate 0.000000%\n",
      "At iteration 45000, loss 0.003145, train error rate 0.000000%\n",
      "At iteration 46000, loss 0.003025, train error rate 0.000000%\n",
      "At iteration 47000, loss 0.002913, train error rate 0.000000%\n",
      "At iteration 48000, loss 0.002809, train error rate 0.000000%\n",
      "Tolerance level reached exiting\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Here we verify that the network can be trained on Irises.\n",
    "# Most runs should result in 100% accurracy\n",
    "#\n",
    "\n",
    "net = FeedForwardNet([\n",
    "        AffineLayer(4,10),\n",
    "        TanhLayer(),\n",
    "        AffineLayer(10,3),\n",
    "        SoftMaxLayer()\n",
    "        ])\n",
    "GD(net, IrisX,IrisY, 1e-1, tolerance=1e-7, max_iters=50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD\n",
    "\n",
    "Your job is to implement SGD training on MNIST with the following elements:\n",
    "1. SGD + momentum\n",
    "2. weight decay\n",
    "3. early stopping\n",
    "\n",
    "In overall, you should get below 2% trainig errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data from Fuel\n",
    "\n",
    "The following cell prepares the data pipeline in fuel. please see SGD template for usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fuel.datasets.mnist import MNIST\n",
    "from fuel.transformers import ScaleAndShift, Cast, Flatten, Mapping\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import SequentialScheme, ShuffledScheme\n",
    "\n",
    "MNIST.default_transformers = (\n",
    "    (ScaleAndShift, [2.0 / 255.0, -1], {'which_sources': 'features'}),\n",
    "    (Cast, [np.float32], {'which_sources': 'features'}), \n",
    "    (Flatten, [], {'which_sources': 'features'}),\n",
    "    (Mapping, [lambda batch: (b.T for b in batch)], {}) )\n",
    "\n",
    "mnist_train = MNIST((\"train\",), subset=slice(None,50000))\n",
    "#this stream will shuffle the MNIST set and return us batches of 100 examples\n",
    "mnist_train_stream = DataStream.default_stream(\n",
    "    mnist_train,\n",
    "    iteration_scheme=ShuffledScheme(mnist_train.num_examples, 100))\n",
    "                                               \n",
    "mnist_validation = MNIST((\"train\",), subset=slice(50000, None))\n",
    "\n",
    "# We will use larger portions for testing and validation\n",
    "# as these dont do a backward pass and reauire less RAM.\n",
    "mnist_validation_stream = DataStream.default_stream(\n",
    "    mnist_validation, iteration_scheme=SequentialScheme(mnist_validation.num_examples, 250))\n",
    "mnist_test = MNIST((\"test\",))\n",
    "mnist_test_stream = DataStream.default_stream(\n",
    "    mnist_test, iteration_scheme=SequentialScheme(mnist_test.num_examples, 250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The streams return batches containing (u'features', u'targets')\n",
      "Each trainin batch consits of a tuple containing:\n",
      " - an array of size (784, 100) containing float32\n",
      " - an array of size (1, 100) containing uint8\n",
      "Validation/test batches consits of tuples containing:\n",
      " - an array of size (784, 250) containing float32\n",
      " - an array of size (1, 250) containing uint8\n"
     ]
    }
   ],
   "source": [
    "print \"The streams return batches containing %s\" % (mnist_train_stream.sources,)\n",
    "\n",
    "print \"Each trainin batch consits of a tuple containing:\"\n",
    "for element in next(mnist_train_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)\n",
    "    \n",
    "print \"Validation/test batches consits of tuples containing:\"\n",
    "for element in next(mnist_test_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Please note, the code blow is able to train a SoftMax regression model on mnist to poor results (ca 8%test error), \n",
    "# you must improve it\n",
    "#\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "def compute_error_rate(net, stream):\n",
    "    num_errs = 0.0\n",
    "    num_examples = 0\n",
    "    for X, Y in stream.get_epoch_iterator():\n",
    "        O = net.fprop(X)\n",
    "        num_errs += (O.argmax(0) != Y).sum()\n",
    "        num_examples += X.shape[1]\n",
    "    return num_errs/num_examples\n",
    "\n",
    "def SGD(net, train_stream, validation_stream, test_stream):\n",
    "    i=0\n",
    "    e=0\n",
    "    \n",
    "    #initialize momentum variables\n",
    "    # Hint: you need one valocity matrix for each parameter\n",
    "    velocities = [np.zeros(P.shape) for P in net.parameters]\n",
    "    best_valid_error_rate = np.inf\n",
    "    best_params = deepcopy(net.parameters)\n",
    "    best_params_epoch = 0\n",
    "    \n",
    "    train_erros = []\n",
    "    train_loss = []\n",
    "    validation_errors = []\n",
    "    \n",
    "    number_of_epochs = 3\n",
    "    patience_expansion = 1.5\n",
    "    \n",
    "    try:\n",
    "        while e<number_of_epochs: #This loop goes over epochs\n",
    "            e += 1\n",
    "            print \"log\", 1.0-1.0/log(e)\n",
    "            #First train on all data from this batch\n",
    "            for X,Y in train_stream.get_epoch_iterator(): \n",
    "                i += 1\n",
    "                L, O, gradients = net.get_cost_and_gradient(X, Y)\n",
    "                err_rate = (O.argmax(0) != Y).mean()\n",
    "                train_loss.append((i,L))\n",
    "                train_erros.append((i,err_rate))\n",
    "                if i % 100 == 0:\n",
    "                    print \"At minibatch %d, batch loss %f, batch error rate %f%%\" % (i, L, err_rate*100)\n",
    "                #print ''\n",
    "                #print \"X\", X.shape\n",
    "                #print \"Y\", Y.shape\n",
    "                for P, V, G, N in zip(net.parameters, velocities, gradients, net.parameter_names):\n",
    "                    #print \"P\",P.shape\n",
    "                    #print \"V\",V.shape\n",
    "                    #print \"G\",G.shape\n",
    "                    #print \"N\",N\n",
    "                    if N=='W':\n",
    "                        # TODO: implement the weight decay addition to gradient\n",
    "                        G += 0.0001 * P\n",
    "                    \n",
    "                    # TODO: set a learning rate\n",
    "                    # Hint, use the iteration counter i\n",
    "                    alpha = 0.1/e#10**-np.maximum(log(e),0.01)\n",
    "                    #alpha = 0.01/log(e) #np.maximum(log(e),1.0)\n",
    "                    \n",
    "                    \n",
    "                    # TODO: set the momentum constant \n",
    "                    epsilon = np.maximum(1.0-1.0/log(e),0.5)\n",
    "                    \n",
    "                    \n",
    "                    # TODO: implement velocity update in momentum\n",
    "                    V[...] = epsilon*V-alpha*G\n",
    "                    \n",
    "                    \n",
    "                    # TODO: set a more sensible learning rule here,\n",
    "                    # using your learning rate schedule and momentum\n",
    "                    #!!!!! Need to modify the actual parameter here! \n",
    "                    #P += -5e-2 * G\n",
    "                    P += V\n",
    "                    \n",
    "            # After an epoch compute validation error\n",
    "            val_error_rate = compute_error_rate(net, validation_stream)\n",
    "            if val_error_rate < best_valid_error_rate:\n",
    "                number_of_epochs = np.maximum(number_of_epochs, e * patience_expansion+1)\n",
    "                best_valid_error_rate = val_error_rate\n",
    "                best_params = deepcopy(net.parameters)\n",
    "                best_params_epoch = e\n",
    "                validation_errors.append((i,val_error_rate))\n",
    "            print \"After epoch %d: valid_err_rate: %f%% currently going ot do %d epochs\" %(\n",
    "                e, val_error_rate*100.0, number_of_epochs)\n",
    "        \n",
    "        print \"Setting network parameters from after epoch %d\" %(best_params_epoch)\n",
    "        net.parameters = best_params\n",
    "    except KeyboardInterrupt:\n",
    "        print \"Setting network parameters from after epoch %d\" %(best_params_epoch)\n",
    "        net.parameters = best_params\n",
    "        \n",
    "        subplot(2,1,1)\n",
    "        train_loss = np.array(train_loss)\n",
    "        semilogy(train_loss[:,0], train_loss[:,1], label='batch train loss')\n",
    "        legend()\n",
    "        \n",
    "        subplot(2,1,2)\n",
    "        train_erros = np.array(train_erros)\n",
    "        plot(train_erros[:,0], train_erros[:,1], label='batch train error rate')\n",
    "        validation_errors = np.array(validation_errors)\n",
    "        plot(validation_errors[:,0], validation_errors[:,1], label='validation error rate', color='r')\n",
    "        ylim(0,0.2)\n",
    "        legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log 0.5\n",
      "At minibatch 100, batch loss 0.508901, batch error rate 13.000000%\n",
      "At minibatch 200, batch loss 0.390935, batch error rate 10.000000%\n",
      "At minibatch 300, batch loss 0.294382, batch error rate 10.000000%\n",
      "At minibatch 400, batch loss 0.269336, batch error rate 12.000000%\n",
      "At minibatch 500, batch loss 0.093404, batch error rate 3.000000%\n",
      "After epoch 1: valid_err_rate: 0.059000% currently going ot do 3 epochs\n",
      "log 0.0\n",
      "At minibatch 600, batch loss 0.131495, batch error rate 6.000000%\n",
      "At minibatch 700, batch loss 0.107259, batch error rate 5.000000%\n",
      "At minibatch 800, batch loss 0.202240, batch error rate 3.000000%\n",
      "At minibatch 900, batch loss 0.129518, batch error rate 3.000000%\n",
      "At minibatch 1000, batch loss 0.076040, batch error rate 2.000000%\n",
      "After epoch 2: valid_err_rate: 0.032500% currently going ot do 4 epochs\n",
      "log -0.5\n",
      "At minibatch 1100, batch loss 0.047668, batch error rate 1.000000%\n",
      "At minibatch 1200, batch loss 0.177228, batch error rate 8.000000%\n",
      "At minibatch 1300, batch loss 0.056911, batch error rate 1.000000%\n",
      "At minibatch 1400, batch loss 0.066632, batch error rate 2.000000%\n",
      "At minibatch 1500, batch loss 0.085944, batch error rate 3.000000%\n",
      "After epoch 3: valid_err_rate: 0.024600% currently going ot do 5 epochs\n",
      "log -1.0\n",
      "At minibatch 1600, batch loss 0.094725, batch error rate 3.000000%\n",
      "At minibatch 1700, batch loss 0.039853, batch error rate 1.000000%\n",
      "At minibatch 1800, batch loss 0.055198, batch error rate 2.000000%\n",
      "At minibatch 1900, batch loss 0.104243, batch error rate 2.000000%\n",
      "At minibatch 2000, batch loss 0.050020, batch error rate 2.000000%\n",
      "After epoch 4: valid_err_rate: 0.023400% currently going ot do 7 epochs\n",
      "log -1.5\n",
      "At minibatch 2100, batch loss 0.015828, batch error rate 0.000000%\n",
      "At minibatch 2200, batch loss 0.041275, batch error rate 2.000000%\n",
      "At minibatch 2300, batch loss 0.039597, batch error rate 1.000000%\n",
      "At minibatch 2400, batch loss 0.106629, batch error rate 3.000000%\n",
      "At minibatch 2500, batch loss 0.035204, batch error rate 0.000000%\n",
      "After epoch 5: valid_err_rate: 0.022300% currently going ot do 8 epochs\n",
      "log -2.0\n",
      "At minibatch 2600, batch loss 0.045804, batch error rate 1.000000%\n",
      "At minibatch 2700, batch loss 0.037222, batch error rate 0.000000%\n",
      "At minibatch 2800, batch loss 0.017582, batch error rate 0.000000%\n",
      "At minibatch 2900, batch loss 0.036207, batch error rate 2.000000%\n",
      "At minibatch 3000, batch loss 0.027988, batch error rate 1.000000%\n",
      "After epoch 6: valid_err_rate: 0.021600% currently going ot do 10 epochs\n",
      "log -2.5\n",
      "At minibatch 3100, batch loss 0.023207, batch error rate 0.000000%\n",
      "At minibatch 3200, batch loss 0.101323, batch error rate 3.000000%\n",
      "At minibatch 3300, batch loss 0.027640, batch error rate 1.000000%\n",
      "At minibatch 3400, batch loss 0.066337, batch error rate 2.000000%\n",
      "At minibatch 3500, batch loss 0.074199, batch error rate 2.000000%\n",
      "After epoch 7: valid_err_rate: 0.020100% currently going ot do 11 epochs\n",
      "log -3.0\n",
      "At minibatch 3600, batch loss 0.037345, batch error rate 1.000000%\n",
      "At minibatch 3700, batch loss 0.027250, batch error rate 0.000000%\n",
      "At minibatch 3800, batch loss 0.013458, batch error rate 0.000000%\n",
      "At minibatch 3900, batch loss 0.051510, batch error rate 2.000000%\n",
      "At minibatch 4000, batch loss 0.032546, batch error rate 1.000000%\n",
      "After epoch 8: valid_err_rate: 0.021000% currently going ot do 11 epochs\n",
      "log -3.5\n",
      "At minibatch 4100, batch loss 0.026702, batch error rate 0.000000%\n",
      "At minibatch 4200, batch loss 0.024133, batch error rate 0.000000%\n",
      "At minibatch 4300, batch loss 0.030280, batch error rate 1.000000%\n",
      "At minibatch 4400, batch loss 0.030698, batch error rate 0.000000%\n",
      "At minibatch 4500, batch loss 0.018803, batch error rate 0.000000%\n",
      "After epoch 9: valid_err_rate: 0.020700% currently going ot do 11 epochs\n",
      "log -4.0\n",
      "At minibatch 4600, batch loss 0.037334, batch error rate 1.000000%\n",
      "At minibatch 4700, batch loss 0.055703, batch error rate 2.000000%\n",
      "At minibatch 4800, batch loss 0.015859, batch error rate 0.000000%\n",
      "At minibatch 4900, batch loss 0.020621, batch error rate 0.000000%\n",
      "At minibatch 5000, batch loss 0.026470, batch error rate 1.000000%\n",
      "After epoch 10: valid_err_rate: 0.019200% currently going ot do 16 epochs\n",
      "log -4.5\n",
      "At minibatch 5100, batch loss 0.023437, batch error rate 0.000000%\n",
      "At minibatch 5200, batch loss 0.028881, batch error rate 1.000000%\n",
      "At minibatch 5300, batch loss 0.015727, batch error rate 0.000000%\n",
      "At minibatch 5400, batch loss 0.033233, batch error rate 1.000000%\n",
      "At minibatch 5500, batch loss 0.060465, batch error rate 1.000000%\n",
      "After epoch 11: valid_err_rate: 0.019500% currently going ot do 16 epochs\n",
      "log -5.0\n",
      "At minibatch 5600, batch loss 0.094082, batch error rate 3.000000%\n",
      "At minibatch 5700, batch loss 0.042734, batch error rate 2.000000%\n",
      "At minibatch 5800, batch loss 0.034592, batch error rate 1.000000%\n",
      "At minibatch 5900, batch loss 0.016339, batch error rate 1.000000%\n",
      "At minibatch 6000, batch loss 0.044063, batch error rate 1.000000%\n",
      "After epoch 12: valid_err_rate: 0.019600% currently going ot do 16 epochs\n",
      "log -5.5\n",
      "At minibatch 6100, batch loss 0.014292, batch error rate 0.000000%\n",
      "At minibatch 6200, batch loss 0.041129, batch error rate 2.000000%\n",
      "At minibatch 6300, batch loss 0.040120, batch error rate 0.000000%\n",
      "At minibatch 6400, batch loss 0.016501, batch error rate 0.000000%\n",
      "At minibatch 6500, batch loss 0.049883, batch error rate 2.000000%\n",
      "After epoch 13: valid_err_rate: 0.021000% currently going ot do 16 epochs\n",
      "log -6.0\n",
      "At minibatch 6600, batch loss 0.046642, batch error rate 2.000000%\n",
      "At minibatch 6700, batch loss 0.016111, batch error rate 0.000000%\n",
      "At minibatch 6800, batch loss 0.016492, batch error rate 0.000000%\n",
      "At minibatch 6900, batch loss 0.020796, batch error rate 1.000000%\n",
      "At minibatch 7000, batch loss 0.018934, batch error rate 1.000000%\n",
      "After epoch 14: valid_err_rate: 0.018900% currently going ot do 22 epochs\n",
      "log -6.5\n",
      "At minibatch 7100, batch loss 0.009619, batch error rate 0.000000%\n",
      "At minibatch 7200, batch loss 0.023819, batch error rate 1.000000%\n",
      "At minibatch 7300, batch loss 0.012015, batch error rate 0.000000%\n",
      "At minibatch 7400, batch loss 0.044388, batch error rate 1.000000%\n",
      "At minibatch 7500, batch loss 0.010122, batch error rate 0.000000%\n",
      "After epoch 15: valid_err_rate: 0.019500% currently going ot do 22 epochs\n",
      "log -7.0\n",
      "At minibatch 7600, batch loss 0.024768, batch error rate 0.000000%\n",
      "At minibatch 7700, batch loss 0.023080, batch error rate 1.000000%\n",
      "At minibatch 7800, batch loss 0.017673, batch error rate 1.000000%\n",
      "At minibatch 7900, batch loss 0.017386, batch error rate 0.000000%\n",
      "At minibatch 8000, batch loss 0.020253, batch error rate 0.000000%\n",
      "After epoch 16: valid_err_rate: 0.018100% currently going ot do 25 epochs\n",
      "log -7.5\n",
      "At minibatch 8100, batch loss 0.024636, batch error rate 1.000000%\n",
      "At minibatch 8200, batch loss 0.020719, batch error rate 0.000000%\n",
      "At minibatch 8300, batch loss 0.044342, batch error rate 1.000000%\n",
      "At minibatch 8400, batch loss 0.007465, batch error rate 0.000000%\n",
      "At minibatch 8500, batch loss 0.005920, batch error rate 0.000000%\n",
      "After epoch 17: valid_err_rate: 0.019000% currently going ot do 25 epochs\n",
      "log -8.0\n",
      "At minibatch 8600, batch loss 0.031010, batch error rate 1.000000%\n",
      "At minibatch 8700, batch loss 0.018969, batch error rate 1.000000%\n",
      "At minibatch 8800, batch loss 0.009445, batch error rate 0.000000%\n",
      "At minibatch 8900, batch loss 0.011828, batch error rate 0.000000%\n",
      "At minibatch 9000, batch loss 0.010590, batch error rate 0.000000%\n",
      "After epoch 18: valid_err_rate: 0.018600% currently going ot do 25 epochs\n",
      "log -8.5\n",
      "At minibatch 9100, batch loss 0.028313, batch error rate 1.000000%\n",
      "At minibatch 9200, batch loss 0.017576, batch error rate 1.000000%\n",
      "At minibatch 9300, batch loss 0.013318, batch error rate 0.000000%\n",
      "At minibatch 9400, batch loss 0.017146, batch error rate 0.000000%\n",
      "At minibatch 9500, batch loss 0.015212, batch error rate 0.000000%\n",
      "After epoch 19: valid_err_rate: 0.018300% currently going ot do 25 epochs\n",
      "log -9.0\n",
      "At minibatch 9600, batch loss 0.007652, batch error rate 0.000000%\n",
      "At minibatch 9700, batch loss 0.025726, batch error rate 1.000000%\n",
      "At minibatch 9800, batch loss 0.036511, batch error rate 2.000000%\n",
      "At minibatch 9900, batch loss 0.014859, batch error rate 0.000000%\n",
      "At minibatch 10000, batch loss 0.014497, batch error rate 1.000000%\n",
      "After epoch 20: valid_err_rate: 0.018300% currently going ot do 25 epochs\n",
      "log -9.5\n",
      "At minibatch 10100, batch loss 0.019818, batch error rate 0.000000%\n",
      "At minibatch 10200, batch loss 0.023339, batch error rate 1.000000%\n",
      "At minibatch 10300, batch loss 0.023352, batch error rate 0.000000%\n",
      "At minibatch 10400, batch loss 0.011041, batch error rate 0.000000%\n",
      "At minibatch 10500, batch loss 0.020027, batch error rate 0.000000%\n",
      "After epoch 21: valid_err_rate: 0.017900% currently going ot do 32 epochs\n",
      "log -10.0\n",
      "At minibatch 10600, batch loss 0.017535, batch error rate 0.000000%\n",
      "At minibatch 10700, batch loss 0.009720, batch error rate 0.000000%\n",
      "At minibatch 10800, batch loss 0.015132, batch error rate 0.000000%\n",
      "At minibatch 10900, batch loss 0.035232, batch error rate 2.000000%\n",
      "At minibatch 11000, batch loss 0.026477, batch error rate 0.000000%\n",
      "After epoch 22: valid_err_rate: 0.017900% currently going ot do 32 epochs\n",
      "log -10.5\n",
      "At minibatch 11100, batch loss 0.012894, batch error rate 0.000000%\n",
      "At minibatch 11200, batch loss 0.015575, batch error rate 0.000000%\n",
      "At minibatch 11300, batch loss 0.028674, batch error rate 0.000000%\n",
      "At minibatch 11400, batch loss 0.010029, batch error rate 0.000000%\n",
      "At minibatch 11500, batch loss 0.016711, batch error rate 0.000000%\n",
      "After epoch 23: valid_err_rate: 0.018000% currently going ot do 32 epochs\n",
      "log -11.0\n",
      "At minibatch 11600, batch loss 0.019101, batch error rate 0.000000%\n",
      "At minibatch 11700, batch loss 0.020353, batch error rate 0.000000%\n",
      "At minibatch 11800, batch loss 0.020564, batch error rate 1.000000%\n",
      "At minibatch 11900, batch loss 0.016442, batch error rate 0.000000%\n",
      "At minibatch 12000, batch loss 0.013687, batch error rate 0.000000%\n",
      "After epoch 24: valid_err_rate: 0.018500% currently going ot do 32 epochs\n",
      "log -11.5\n",
      "At minibatch 12100, batch loss 0.030072, batch error rate 1.000000%\n",
      "At minibatch 12200, batch loss 0.005721, batch error rate 0.000000%\n",
      "At minibatch 12300, batch loss 0.012187, batch error rate 0.000000%\n",
      "At minibatch 12400, batch loss 0.042157, batch error rate 1.000000%\n",
      "At minibatch 12500, batch loss 0.050569, batch error rate 3.000000%\n",
      "After epoch 25: valid_err_rate: 0.017700% currently going ot do 38 epochs\n",
      "log -12.0\n",
      "At minibatch 12600, batch loss 0.016556, batch error rate 0.000000%\n",
      "At minibatch 12700, batch loss 0.009618, batch error rate 0.000000%\n",
      "At minibatch 12800, batch loss 0.016477, batch error rate 0.000000%\n",
      "At minibatch 12900, batch loss 0.008890, batch error rate 0.000000%\n",
      "At minibatch 13000, batch loss 0.006199, batch error rate 0.000000%\n",
      "After epoch 26: valid_err_rate: 0.017700% currently going ot do 38 epochs\n",
      "log -12.5\n",
      "At minibatch 13100, batch loss 0.022924, batch error rate 0.000000%\n",
      "At minibatch 13200, batch loss 0.013604, batch error rate 0.000000%\n",
      "At minibatch 13300, batch loss 0.033175, batch error rate 0.000000%\n",
      "At minibatch 13400, batch loss 0.075516, batch error rate 2.000000%\n",
      "At minibatch 13500, batch loss 0.013942, batch error rate 0.000000%\n",
      "After epoch 27: valid_err_rate: 0.017900% currently going ot do 38 epochs\n",
      "log -13.0\n",
      "At minibatch 13600, batch loss 0.018023, batch error rate 0.000000%\n",
      "At minibatch 13700, batch loss 0.013842, batch error rate 0.000000%\n",
      "At minibatch 13800, batch loss 0.012842, batch error rate 0.000000%\n",
      "At minibatch 13900, batch loss 0.017732, batch error rate 0.000000%\n",
      "At minibatch 14000, batch loss 0.006608, batch error rate 0.000000%\n",
      "After epoch 28: valid_err_rate: 0.018300% currently going ot do 38 epochs\n",
      "log -13.5\n",
      "At minibatch 14100, batch loss 0.010316, batch error rate 0.000000%\n",
      "At minibatch 14200, batch loss 0.009293, batch error rate 0.000000%\n",
      "At minibatch 14300, batch loss 0.016151, batch error rate 0.000000%\n",
      "At minibatch 14400, batch loss 0.028491, batch error rate 1.000000%\n",
      "At minibatch 14500, batch loss 0.022744, batch error rate 0.000000%\n",
      "After epoch 29: valid_err_rate: 0.017600% currently going ot do 44 epochs\n",
      "log -14.0\n",
      "At minibatch 14600, batch loss 0.023909, batch error rate 1.000000%\n",
      "At minibatch 14700, batch loss 0.020980, batch error rate 0.000000%\n",
      "At minibatch 14800, batch loss 0.021787, batch error rate 0.000000%\n",
      "At minibatch 14900, batch loss 0.009160, batch error rate 0.000000%\n",
      "At minibatch 15000, batch loss 0.021196, batch error rate 0.000000%\n",
      "After epoch 30: valid_err_rate: 0.018300% currently going ot do 44 epochs\n",
      "log -14.5\n",
      "At minibatch 15100, batch loss 0.011237, batch error rate 0.000000%\n",
      "At minibatch 15200, batch loss 0.008905, batch error rate 0.000000%\n",
      "At minibatch 15300, batch loss 0.016293, batch error rate 0.000000%\n",
      "At minibatch 15400, batch loss 0.010622, batch error rate 0.000000%\n",
      "At minibatch 15500, batch loss 0.038356, batch error rate 2.000000%\n",
      "After epoch 31: valid_err_rate: 0.017500% currently going ot do 47 epochs\n",
      "log -15.0\n",
      "At minibatch 15600, batch loss 0.009016, batch error rate 0.000000%\n",
      "At minibatch 15700, batch loss 0.018675, batch error rate 1.000000%\n",
      "At minibatch 15800, batch loss 0.007101, batch error rate 0.000000%\n",
      "At minibatch 15900, batch loss 0.010889, batch error rate 0.000000%\n",
      "At minibatch 16000, batch loss 0.029340, batch error rate 1.000000%\n",
      "After epoch 32: valid_err_rate: 0.017800% currently going ot do 47 epochs\n",
      "log -15.5\n",
      "At minibatch 16100, batch loss 0.014530, batch error rate 0.000000%\n",
      "At minibatch 16200, batch loss 0.014073, batch error rate 0.000000%\n",
      "At minibatch 16300, batch loss 0.022210, batch error rate 1.000000%\n",
      "At minibatch 16400, batch loss 0.015937, batch error rate 0.000000%\n",
      "At minibatch 16500, batch loss 0.021881, batch error rate 0.000000%\n",
      "After epoch 33: valid_err_rate: 0.018000% currently going ot do 47 epochs\n",
      "log -16.0\n",
      "At minibatch 16600, batch loss 0.016865, batch error rate 0.000000%\n",
      "At minibatch 16700, batch loss 0.008073, batch error rate 0.000000%\n",
      "At minibatch 16800, batch loss 0.007115, batch error rate 0.000000%\n",
      "At minibatch 16900, batch loss 0.014512, batch error rate 0.000000%\n",
      "At minibatch 17000, batch loss 0.012278, batch error rate 0.000000%\n",
      "After epoch 34: valid_err_rate: 0.017400% currently going ot do 52 epochs\n",
      "log -16.5\n",
      "At minibatch 17100, batch loss 0.013046, batch error rate 0.000000%\n",
      "At minibatch 17200, batch loss 0.006372, batch error rate 0.000000%\n",
      "At minibatch 17300, batch loss 0.030854, batch error rate 0.000000%\n",
      "At minibatch 17400, batch loss 0.009652, batch error rate 0.000000%\n",
      "At minibatch 17500, batch loss 0.008099, batch error rate 0.000000%\n",
      "After epoch 35: valid_err_rate: 0.017900% currently going ot do 52 epochs\n",
      "log -17.0\n",
      "At minibatch 17600, batch loss 0.008209, batch error rate 0.000000%\n",
      "At minibatch 17700, batch loss 0.010685, batch error rate 0.000000%\n",
      "At minibatch 17800, batch loss 0.015407, batch error rate 0.000000%\n",
      "At minibatch 17900, batch loss 0.013100, batch error rate 0.000000%\n",
      "At minibatch 18000, batch loss 0.018771, batch error rate 0.000000%\n",
      "After epoch 36: valid_err_rate: 0.017200% currently going ot do 55 epochs\n",
      "log -17.5\n",
      "At minibatch 18100, batch loss 0.016491, batch error rate 0.000000%\n",
      "At minibatch 18200, batch loss 0.006628, batch error rate 0.000000%\n",
      "At minibatch 18300, batch loss 0.011348, batch error rate 0.000000%\n",
      "At minibatch 18400, batch loss 0.007034, batch error rate 0.000000%\n",
      "At minibatch 18500, batch loss 0.010137, batch error rate 0.000000%\n",
      "After epoch 37: valid_err_rate: 0.017300% currently going ot do 55 epochs\n",
      "log -18.0\n",
      "At minibatch 18600, batch loss 0.008366, batch error rate 0.000000%\n",
      "At minibatch 18700, batch loss 0.008295, batch error rate 0.000000%\n",
      "At minibatch 18800, batch loss 0.011480, batch error rate 0.000000%\n",
      "At minibatch 18900, batch loss 0.011922, batch error rate 0.000000%\n",
      "At minibatch 19000, batch loss 0.014789, batch error rate 0.000000%\n",
      "After epoch 38: valid_err_rate: 0.017200% currently going ot do 55 epochs\n",
      "log -18.5\n",
      "At minibatch 19100, batch loss 0.027320, batch error rate 1.000000%\n",
      "At minibatch 19200, batch loss 0.008184, batch error rate 0.000000%\n",
      "At minibatch 19300, batch loss 0.006654, batch error rate 0.000000%\n",
      "At minibatch 19400, batch loss 0.005078, batch error rate 0.000000%\n",
      "At minibatch 19500, batch loss 0.017279, batch error rate 0.000000%\n",
      "After epoch 39: valid_err_rate: 0.017200% currently going ot do 55 epochs\n",
      "log -19.0\n",
      "At minibatch 19600, batch loss 0.032825, batch error rate 1.000000%\n",
      "At minibatch 19700, batch loss 0.013169, batch error rate 0.000000%\n",
      "At minibatch 19800, batch loss 0.011522, batch error rate 0.000000%\n",
      "At minibatch 19900, batch loss 0.008547, batch error rate 0.000000%\n",
      "At minibatch 20000, batch loss 0.010177, batch error rate 0.000000%\n",
      "After epoch 40: valid_err_rate: 0.018000% currently going ot do 55 epochs\n",
      "log -19.5\n",
      "At minibatch 20100, batch loss 0.013839, batch error rate 0.000000%\n",
      "At minibatch 20200, batch loss 0.012364, batch error rate 0.000000%\n",
      "At minibatch 20300, batch loss 0.016186, batch error rate 0.000000%\n",
      "At minibatch 20400, batch loss 0.009011, batch error rate 0.000000%\n",
      "At minibatch 20500, batch loss 0.005456, batch error rate 0.000000%\n",
      "After epoch 41: valid_err_rate: 0.017500% currently going ot do 55 epochs\n",
      "log -20.0\n",
      "At minibatch 20600, batch loss 0.017887, batch error rate 0.000000%\n",
      "At minibatch 20700, batch loss 0.006035, batch error rate 0.000000%\n",
      "At minibatch 20800, batch loss 0.007344, batch error rate 0.000000%\n",
      "At minibatch 20900, batch loss 0.010444, batch error rate 0.000000%\n",
      "At minibatch 21000, batch loss 0.013608, batch error rate 0.000000%\n",
      "After epoch 42: valid_err_rate: 0.017300% currently going ot do 55 epochs\n",
      "log -20.5\n",
      "At minibatch 21100, batch loss 0.010389, batch error rate 0.000000%\n",
      "At minibatch 21200, batch loss 0.009689, batch error rate 0.000000%\n",
      "At minibatch 21300, batch loss 0.023252, batch error rate 0.000000%\n",
      "At minibatch 21400, batch loss 0.019512, batch error rate 0.000000%\n",
      "At minibatch 21500, batch loss 0.024995, batch error rate 0.000000%\n",
      "After epoch 43: valid_err_rate: 0.017100% currently going ot do 65 epochs\n",
      "log -21.0\n",
      "At minibatch 21600, batch loss 0.009568, batch error rate 0.000000%\n",
      "At minibatch 21700, batch loss 0.012582, batch error rate 0.000000%\n",
      "At minibatch 21800, batch loss 0.009068, batch error rate 0.000000%\n",
      "At minibatch 21900, batch loss 0.006611, batch error rate 0.000000%\n",
      "At minibatch 22000, batch loss 0.005789, batch error rate 0.000000%\n",
      "After epoch 44: valid_err_rate: 0.017500% currently going ot do 65 epochs\n",
      "log -21.5\n",
      "At minibatch 22100, batch loss 0.024168, batch error rate 0.000000%\n",
      "At minibatch 22200, batch loss 0.007254, batch error rate 0.000000%\n",
      "At minibatch 22300, batch loss 0.027572, batch error rate 1.000000%\n",
      "At minibatch 22400, batch loss 0.007878, batch error rate 0.000000%\n",
      "At minibatch 22500, batch loss 0.018678, batch error rate 0.000000%\n",
      "After epoch 45: valid_err_rate: 0.017500% currently going ot do 65 epochs\n",
      "log -22.0\n",
      "At minibatch 22600, batch loss 0.012192, batch error rate 1.000000%\n",
      "At minibatch 22700, batch loss 0.051967, batch error rate 2.000000%\n",
      "At minibatch 22800, batch loss 0.013082, batch error rate 0.000000%\n",
      "At minibatch 22900, batch loss 0.010555, batch error rate 0.000000%\n",
      "At minibatch 23000, batch loss 0.042603, batch error rate 1.000000%\n",
      "After epoch 46: valid_err_rate: 0.017000% currently going ot do 70 epochs\n",
      "log -22.5\n",
      "At minibatch 23100, batch loss 0.008487, batch error rate 0.000000%\n",
      "At minibatch 23200, batch loss 0.006281, batch error rate 0.000000%\n",
      "At minibatch 23300, batch loss 0.008592, batch error rate 0.000000%\n",
      "At minibatch 23400, batch loss 0.013373, batch error rate 0.000000%\n",
      "At minibatch 23500, batch loss 0.026246, batch error rate 1.000000%\n",
      "After epoch 47: valid_err_rate: 0.017500% currently going ot do 70 epochs\n",
      "log -23.0\n",
      "At minibatch 23600, batch loss 0.022107, batch error rate 1.000000%\n",
      "At minibatch 23700, batch loss 0.016584, batch error rate 0.000000%\n",
      "At minibatch 23800, batch loss 0.025986, batch error rate 1.000000%\n",
      "At minibatch 23900, batch loss 0.008013, batch error rate 0.000000%\n",
      "At minibatch 24000, batch loss 0.027142, batch error rate 1.000000%\n",
      "After epoch 48: valid_err_rate: 0.017100% currently going ot do 70 epochs\n",
      "log -23.5\n",
      "At minibatch 24100, batch loss 0.016965, batch error rate 0.000000%\n",
      "At minibatch 24200, batch loss 0.003441, batch error rate 0.000000%\n",
      "At minibatch 24300, batch loss 0.036622, batch error rate 2.000000%\n",
      "At minibatch 24400, batch loss 0.018076, batch error rate 0.000000%\n",
      "At minibatch 24500, batch loss 0.013678, batch error rate 0.000000%\n",
      "After epoch 49: valid_err_rate: 0.017600% currently going ot do 70 epochs\n",
      "log -24.0\n",
      "At minibatch 24600, batch loss 0.013430, batch error rate 0.000000%\n",
      "At minibatch 24700, batch loss 0.003852, batch error rate 0.000000%\n",
      "At minibatch 24800, batch loss 0.025812, batch error rate 1.000000%\n",
      "At minibatch 24900, batch loss 0.013549, batch error rate 0.000000%\n",
      "At minibatch 25000, batch loss 0.008331, batch error rate 0.000000%\n",
      "After epoch 50: valid_err_rate: 0.017000% currently going ot do 70 epochs\n",
      "log -24.5\n",
      "At minibatch 25100, batch loss 0.017021, batch error rate 0.000000%\n",
      "At minibatch 25200, batch loss 0.011731, batch error rate 0.000000%\n",
      "At minibatch 25300, batch loss 0.016363, batch error rate 0.000000%\n",
      "At minibatch 25400, batch loss 0.007722, batch error rate 0.000000%\n",
      "At minibatch 25500, batch loss 0.009597, batch error rate 0.000000%\n",
      "After epoch 51: valid_err_rate: 0.017700% currently going ot do 70 epochs\n",
      "log -25.0\n",
      "At minibatch 25600, batch loss 0.005907, batch error rate 0.000000%\n",
      "At minibatch 25700, batch loss 0.016397, batch error rate 0.000000%\n",
      "At minibatch 25800, batch loss 0.008236, batch error rate 0.000000%\n",
      "At minibatch 25900, batch loss 0.005928, batch error rate 0.000000%\n",
      "At minibatch 26000, batch loss 0.005930, batch error rate 0.000000%\n",
      "After epoch 52: valid_err_rate: 0.017400% currently going ot do 70 epochs\n",
      "log -25.5\n",
      "At minibatch 26100, batch loss 0.006065, batch error rate 0.000000%\n",
      "At minibatch 26200, batch loss 0.012626, batch error rate 0.000000%\n",
      "At minibatch 26300, batch loss 0.006205, batch error rate 0.000000%\n",
      "At minibatch 26400, batch loss 0.007542, batch error rate 0.000000%\n",
      "At minibatch 26500, batch loss 0.013711, batch error rate 0.000000%\n",
      "After epoch 53: valid_err_rate: 0.017200% currently going ot do 70 epochs\n",
      "log -26.0\n",
      "At minibatch 26600, batch loss 0.016713, batch error rate 0.000000%\n",
      "At minibatch 26700, batch loss 0.019064, batch error rate 1.000000%\n",
      "At minibatch 26800, batch loss 0.010097, batch error rate 0.000000%\n",
      "At minibatch 26900, batch loss 0.018528, batch error rate 0.000000%\n",
      "At minibatch 27000, batch loss 0.012254, batch error rate 0.000000%\n",
      "After epoch 54: valid_err_rate: 0.017100% currently going ot do 70 epochs\n",
      "log -26.5\n",
      "At minibatch 27100, batch loss 0.008459, batch error rate 0.000000%\n",
      "At minibatch 27200, batch loss 0.010399, batch error rate 0.000000%\n",
      "At minibatch 27300, batch loss 0.007976, batch error rate 0.000000%\n",
      "At minibatch 27400, batch loss 0.009157, batch error rate 0.000000%\n",
      "At minibatch 27500, batch loss 0.006740, batch error rate 0.000000%\n",
      "After epoch 55: valid_err_rate: 0.017600% currently going ot do 70 epochs\n",
      "log -27.0\n",
      "At minibatch 27600, batch loss 0.007605, batch error rate 0.000000%\n",
      "At minibatch 27700, batch loss 0.013125, batch error rate 0.000000%\n",
      "At minibatch 27800, batch loss 0.009138, batch error rate 0.000000%\n",
      "At minibatch 27900, batch loss 0.008840, batch error rate 0.000000%\n",
      "At minibatch 28000, batch loss 0.006359, batch error rate 0.000000%\n",
      "After epoch 56: valid_err_rate: 0.017600% currently going ot do 70 epochs\n",
      "log -27.5\n",
      "At minibatch 28100, batch loss 0.008444, batch error rate 0.000000%\n",
      "At minibatch 28200, batch loss 0.012538, batch error rate 0.000000%\n",
      "At minibatch 28300, batch loss 0.040350, batch error rate 1.000000%\n",
      "At minibatch 28400, batch loss 0.015504, batch error rate 0.000000%\n",
      "At minibatch 28500, batch loss 0.011907, batch error rate 0.000000%\n",
      "After epoch 57: valid_err_rate: 0.016700% currently going ot do 86 epochs\n",
      "log -28.0\n",
      "At minibatch 28600, batch loss 0.008682, batch error rate 0.000000%\n",
      "At minibatch 28700, batch loss 0.004383, batch error rate 0.000000%\n",
      "At minibatch 28800, batch loss 0.010698, batch error rate 0.000000%\n",
      "At minibatch 28900, batch loss 0.017771, batch error rate 0.000000%\n",
      "At minibatch 29000, batch loss 0.003638, batch error rate 0.000000%\n",
      "After epoch 58: valid_err_rate: 0.017600% currently going ot do 86 epochs\n",
      "log -28.5\n",
      "At minibatch 29100, batch loss 0.005060, batch error rate 0.000000%\n",
      "At minibatch 29200, batch loss 0.010489, batch error rate 0.000000%\n",
      "At minibatch 29300, batch loss 0.014468, batch error rate 0.000000%\n",
      "At minibatch 29400, batch loss 0.044109, batch error rate 1.000000%\n",
      "At minibatch 29500, batch loss 0.013383, batch error rate 0.000000%\n",
      "After epoch 59: valid_err_rate: 0.018000% currently going ot do 86 epochs\n",
      "log -29.0\n",
      "At minibatch 29600, batch loss 0.008722, batch error rate 0.000000%\n",
      "At minibatch 29700, batch loss 0.013127, batch error rate 0.000000%\n",
      "At minibatch 29800, batch loss 0.005415, batch error rate 0.000000%\n",
      "At minibatch 29900, batch loss 0.011345, batch error rate 0.000000%\n",
      "At minibatch 30000, batch loss 0.037944, batch error rate 1.000000%\n",
      "After epoch 60: valid_err_rate: 0.017300% currently going ot do 86 epochs\n",
      "log -29.5\n",
      "At minibatch 30100, batch loss 0.008618, batch error rate 0.000000%\n",
      "At minibatch 30200, batch loss 0.009453, batch error rate 0.000000%\n",
      "At minibatch 30300, batch loss 0.004890, batch error rate 0.000000%\n",
      "At minibatch 30400, batch loss 0.005930, batch error rate 0.000000%\n",
      "At minibatch 30500, batch loss 0.004209, batch error rate 0.000000%\n",
      "After epoch 61: valid_err_rate: 0.017200% currently going ot do 86 epochs\n",
      "log -30.0\n",
      "At minibatch 30600, batch loss 0.012890, batch error rate 0.000000%\n",
      "At minibatch 30700, batch loss 0.010325, batch error rate 0.000000%\n",
      "At minibatch 30800, batch loss 0.007416, batch error rate 0.000000%\n",
      "At minibatch 30900, batch loss 0.011589, batch error rate 0.000000%\n",
      "At minibatch 31000, batch loss 0.023643, batch error rate 1.000000%\n",
      "After epoch 62: valid_err_rate: 0.017000% currently going ot do 86 epochs\n",
      "log -30.5\n",
      "At minibatch 31100, batch loss 0.007342, batch error rate 0.000000%\n",
      "At minibatch 31200, batch loss 0.045111, batch error rate 1.000000%\n",
      "At minibatch 31300, batch loss 0.006294, batch error rate 0.000000%\n",
      "At minibatch 31400, batch loss 0.007218, batch error rate 0.000000%\n",
      "At minibatch 31500, batch loss 0.012182, batch error rate 0.000000%\n",
      "After epoch 63: valid_err_rate: 0.016800% currently going ot do 86 epochs\n",
      "log -31.0\n",
      "At minibatch 31600, batch loss 0.010966, batch error rate 0.000000%\n",
      "At minibatch 31700, batch loss 0.005353, batch error rate 0.000000%\n",
      "At minibatch 31800, batch loss 0.008275, batch error rate 0.000000%\n",
      "At minibatch 31900, batch loss 0.009564, batch error rate 0.000000%\n",
      "At minibatch 32000, batch loss 0.020630, batch error rate 1.000000%\n",
      "After epoch 64: valid_err_rate: 0.017700% currently going ot do 86 epochs\n",
      "log -31.5\n",
      "At minibatch 32100, batch loss 0.014661, batch error rate 0.000000%\n",
      "At minibatch 32200, batch loss 0.007835, batch error rate 0.000000%\n",
      "At minibatch 32300, batch loss 0.004365, batch error rate 0.000000%\n",
      "At minibatch 32400, batch loss 0.004012, batch error rate 0.000000%\n",
      "At minibatch 32500, batch loss 0.006277, batch error rate 0.000000%\n",
      "After epoch 65: valid_err_rate: 0.017400% currently going ot do 86 epochs\n",
      "log -32.0\n",
      "At minibatch 32600, batch loss 0.007095, batch error rate 0.000000%\n",
      "At minibatch 32700, batch loss 0.016558, batch error rate 0.000000%\n",
      "At minibatch 32800, batch loss 0.018188, batch error rate 0.000000%\n",
      "At minibatch 32900, batch loss 0.007663, batch error rate 0.000000%\n",
      "At minibatch 33000, batch loss 0.004963, batch error rate 0.000000%\n",
      "After epoch 66: valid_err_rate: 0.017300% currently going ot do 86 epochs\n",
      "log -32.5\n",
      "At minibatch 33100, batch loss 0.008283, batch error rate 0.000000%\n",
      "At minibatch 33200, batch loss 0.027604, batch error rate 1.000000%\n",
      "At minibatch 33300, batch loss 0.005323, batch error rate 0.000000%\n",
      "At minibatch 33400, batch loss 0.014879, batch error rate 0.000000%\n",
      "At minibatch 33500, batch loss 0.005458, batch error rate 0.000000%\n",
      "After epoch 67: valid_err_rate: 0.016500% currently going ot do 101 epochs\n",
      "log -33.0\n",
      "At minibatch 33600, batch loss 0.005940, batch error rate 0.000000%\n",
      "At minibatch 33700, batch loss 0.009466, batch error rate 0.000000%\n",
      "At minibatch 33800, batch loss 0.008337, batch error rate 0.000000%\n",
      "At minibatch 33900, batch loss 0.012119, batch error rate 0.000000%\n",
      "At minibatch 34000, batch loss 0.019626, batch error rate 0.000000%\n",
      "After epoch 68: valid_err_rate: 0.017200% currently going ot do 101 epochs\n",
      "log -33.5\n",
      "At minibatch 34100, batch loss 0.008880, batch error rate 0.000000%\n",
      "At minibatch 34200, batch loss 0.007714, batch error rate 0.000000%\n",
      "At minibatch 34300, batch loss 0.007274, batch error rate 0.000000%\n",
      "At minibatch 34400, batch loss 0.009893, batch error rate 0.000000%\n",
      "At minibatch 34500, batch loss 0.033296, batch error rate 1.000000%\n",
      "After epoch 69: valid_err_rate: 0.017400% currently going ot do 101 epochs\n",
      "log -34.0\n",
      "At minibatch 34600, batch loss 0.013296, batch error rate 0.000000%\n",
      "At minibatch 34700, batch loss 0.010608, batch error rate 0.000000%\n",
      "At minibatch 34800, batch loss 0.025036, batch error rate 1.000000%\n",
      "At minibatch 34900, batch loss 0.009305, batch error rate 0.000000%\n",
      "At minibatch 35000, batch loss 0.007815, batch error rate 0.000000%\n",
      "After epoch 70: valid_err_rate: 0.017300% currently going ot do 101 epochs\n",
      "log -34.5\n",
      "At minibatch 35100, batch loss 0.008317, batch error rate 0.000000%\n",
      "At minibatch 35200, batch loss 0.016910, batch error rate 0.000000%\n",
      "At minibatch 35300, batch loss 0.006634, batch error rate 0.000000%\n",
      "At minibatch 35400, batch loss 0.026861, batch error rate 1.000000%\n",
      "At minibatch 35500, batch loss 0.015619, batch error rate 0.000000%\n",
      "After epoch 71: valid_err_rate: 0.016600% currently going ot do 101 epochs\n",
      "log -35.0\n",
      "At minibatch 35600, batch loss 0.003852, batch error rate 0.000000%\n",
      "At minibatch 35700, batch loss 0.007028, batch error rate 0.000000%\n",
      "At minibatch 35800, batch loss 0.015805, batch error rate 0.000000%\n",
      "At minibatch 35900, batch loss 0.006738, batch error rate 0.000000%\n",
      "At minibatch 36000, batch loss 0.005765, batch error rate 0.000000%\n",
      "After epoch 72: valid_err_rate: 0.016800% currently going ot do 101 epochs\n",
      "log -35.5\n",
      "At minibatch 36100, batch loss 0.014332, batch error rate 0.000000%\n",
      "At minibatch 36200, batch loss 0.010666, batch error rate 0.000000%\n",
      "At minibatch 36300, batch loss 0.005793, batch error rate 0.000000%\n",
      "At minibatch 36400, batch loss 0.002914, batch error rate 0.000000%\n",
      "At minibatch 36500, batch loss 0.009828, batch error rate 0.000000%\n",
      "After epoch 73: valid_err_rate: 0.017300% currently going ot do 101 epochs\n",
      "log -36.0\n",
      "At minibatch 36600, batch loss 0.007219, batch error rate 0.000000%\n",
      "At minibatch 36700, batch loss 0.010238, batch error rate 0.000000%\n",
      "At minibatch 36800, batch loss 0.006638, batch error rate 0.000000%\n",
      "At minibatch 36900, batch loss 0.007313, batch error rate 0.000000%\n",
      "At minibatch 37000, batch loss 0.005060, batch error rate 0.000000%\n",
      "After epoch 74: valid_err_rate: 0.017000% currently going ot do 101 epochs\n",
      "log -36.5\n",
      "At minibatch 37100, batch loss 0.012146, batch error rate 0.000000%\n",
      "At minibatch 37200, batch loss 0.011909, batch error rate 0.000000%\n",
      "At minibatch 37300, batch loss 0.003675, batch error rate 0.000000%\n",
      "At minibatch 37400, batch loss 0.015240, batch error rate 0.000000%\n",
      "At minibatch 37500, batch loss 0.007674, batch error rate 0.000000%\n",
      "After epoch 75: valid_err_rate: 0.016500% currently going ot do 101 epochs\n",
      "log -37.0\n",
      "At minibatch 37600, batch loss 0.015816, batch error rate 0.000000%\n",
      "At minibatch 37700, batch loss 0.006064, batch error rate 0.000000%\n",
      "At minibatch 37800, batch loss 0.008111, batch error rate 0.000000%\n",
      "At minibatch 37900, batch loss 0.014962, batch error rate 0.000000%\n",
      "At minibatch 38000, batch loss 0.011701, batch error rate 0.000000%\n",
      "After epoch 76: valid_err_rate: 0.016700% currently going ot do 101 epochs\n",
      "log -37.5\n",
      "At minibatch 38100, batch loss 0.005495, batch error rate 0.000000%\n",
      "At minibatch 38200, batch loss 0.011856, batch error rate 0.000000%\n",
      "At minibatch 38300, batch loss 0.003297, batch error rate 0.000000%\n",
      "At minibatch 38400, batch loss 0.006591, batch error rate 0.000000%\n",
      "At minibatch 38500, batch loss 0.015503, batch error rate 0.000000%\n",
      "After epoch 77: valid_err_rate: 0.016700% currently going ot do 101 epochs\n",
      "log -38.0\n",
      "At minibatch 38600, batch loss 0.003157, batch error rate 0.000000%\n",
      "At minibatch 38700, batch loss 0.010027, batch error rate 0.000000%\n",
      "At minibatch 38800, batch loss 0.008368, batch error rate 0.000000%\n",
      "At minibatch 38900, batch loss 0.014105, batch error rate 0.000000%\n",
      "At minibatch 39000, batch loss 0.008983, batch error rate 0.000000%\n",
      "After epoch 78: valid_err_rate: 0.017000% currently going ot do 101 epochs\n",
      "log -38.5\n",
      "At minibatch 39100, batch loss 0.014681, batch error rate 0.000000%\n",
      "At minibatch 39200, batch loss 0.008824, batch error rate 0.000000%\n",
      "At minibatch 39300, batch loss 0.010108, batch error rate 0.000000%\n",
      "At minibatch 39400, batch loss 0.009396, batch error rate 0.000000%\n",
      "At minibatch 39500, batch loss 0.012800, batch error rate 0.000000%\n",
      "After epoch 79: valid_err_rate: 0.017200% currently going ot do 101 epochs\n",
      "log -39.0\n",
      "At minibatch 39600, batch loss 0.012839, batch error rate 0.000000%\n",
      "At minibatch 39700, batch loss 0.029628, batch error rate 1.000000%\n",
      "At minibatch 39800, batch loss 0.009198, batch error rate 0.000000%\n",
      "At minibatch 39900, batch loss 0.005008, batch error rate 0.000000%\n",
      "At minibatch 40000, batch loss 0.004728, batch error rate 0.000000%\n",
      "After epoch 80: valid_err_rate: 0.017100% currently going ot do 101 epochs\n",
      "log -39.5\n",
      "At minibatch 40100, batch loss 0.009649, batch error rate 0.000000%\n",
      "At minibatch 40200, batch loss 0.010580, batch error rate 0.000000%\n",
      "At minibatch 40300, batch loss 0.008382, batch error rate 0.000000%\n",
      "At minibatch 40400, batch loss 0.012049, batch error rate 0.000000%\n",
      "At minibatch 40500, batch loss 0.010229, batch error rate 0.000000%\n",
      "After epoch 81: valid_err_rate: 0.017000% currently going ot do 101 epochs\n",
      "log -40.0\n",
      "At minibatch 40600, batch loss 0.008816, batch error rate 0.000000%\n",
      "At minibatch 40700, batch loss 0.003824, batch error rate 0.000000%\n",
      "At minibatch 40800, batch loss 0.042161, batch error rate 1.000000%\n",
      "At minibatch 40900, batch loss 0.015263, batch error rate 0.000000%\n",
      "At minibatch 41000, batch loss 0.004699, batch error rate 0.000000%\n",
      "After epoch 82: valid_err_rate: 0.017300% currently going ot do 101 epochs\n",
      "log -40.5\n",
      "At minibatch 41100, batch loss 0.004123, batch error rate 0.000000%\n",
      "At minibatch 41200, batch loss 0.007791, batch error rate 0.000000%\n",
      "At minibatch 41300, batch loss 0.011908, batch error rate 0.000000%\n",
      "At minibatch 41400, batch loss 0.006654, batch error rate 0.000000%\n",
      "At minibatch 41500, batch loss 0.010498, batch error rate 0.000000%\n",
      "After epoch 83: valid_err_rate: 0.016700% currently going ot do 101 epochs\n",
      "log -41.0\n",
      "At minibatch 41600, batch loss 0.012787, batch error rate 0.000000%\n",
      "At minibatch 41700, batch loss 0.015157, batch error rate 0.000000%\n",
      "At minibatch 41800, batch loss 0.012775, batch error rate 0.000000%\n",
      "At minibatch 41900, batch loss 0.005143, batch error rate 0.000000%\n",
      "At minibatch 42000, batch loss 0.012298, batch error rate 0.000000%\n",
      "After epoch 84: valid_err_rate: 0.017100% currently going ot do 101 epochs\n",
      "log -41.5\n",
      "At minibatch 42100, batch loss 0.020272, batch error rate 1.000000%\n",
      "At minibatch 42200, batch loss 0.003485, batch error rate 0.000000%\n",
      "At minibatch 42300, batch loss 0.015159, batch error rate 0.000000%\n",
      "At minibatch 42400, batch loss 0.008886, batch error rate 0.000000%\n",
      "At minibatch 42500, batch loss 0.013849, batch error rate 0.000000%\n",
      "After epoch 85: valid_err_rate: 0.017500% currently going ot do 101 epochs\n",
      "log -42.0\n",
      "At minibatch 42600, batch loss 0.004845, batch error rate 0.000000%\n",
      "At minibatch 42700, batch loss 0.008294, batch error rate 0.000000%\n",
      "At minibatch 42800, batch loss 0.008377, batch error rate 0.000000%\n",
      "At minibatch 42900, batch loss 0.016937, batch error rate 0.000000%\n",
      "At minibatch 43000, batch loss 0.006204, batch error rate 0.000000%\n",
      "After epoch 86: valid_err_rate: 0.016700% currently going ot do 101 epochs\n",
      "log -42.5\n",
      "At minibatch 43100, batch loss 0.011163, batch error rate 0.000000%\n",
      "At minibatch 43200, batch loss 0.003511, batch error rate 0.000000%\n",
      "At minibatch 43300, batch loss 0.002278, batch error rate 0.000000%\n",
      "At minibatch 43400, batch loss 0.004715, batch error rate 0.000000%\n",
      "At minibatch 43500, batch loss 0.011522, batch error rate 0.000000%\n",
      "After epoch 87: valid_err_rate: 0.016800% currently going ot do 101 epochs\n",
      "log -43.0\n",
      "At minibatch 43600, batch loss 0.010301, batch error rate 0.000000%\n",
      "At minibatch 43700, batch loss 0.020209, batch error rate 1.000000%\n",
      "At minibatch 43800, batch loss 0.003973, batch error rate 0.000000%\n",
      "At minibatch 43900, batch loss 0.014060, batch error rate 0.000000%\n",
      "At minibatch 44000, batch loss 0.008829, batch error rate 0.000000%\n",
      "After epoch 88: valid_err_rate: 0.016800% currently going ot do 101 epochs\n",
      "log -43.5\n",
      "At minibatch 44100, batch loss 0.009182, batch error rate 0.000000%\n",
      "At minibatch 44200, batch loss 0.010578, batch error rate 0.000000%\n",
      "At minibatch 44300, batch loss 0.015879, batch error rate 0.000000%\n",
      "At minibatch 44400, batch loss 0.007765, batch error rate 0.000000%\n",
      "At minibatch 44500, batch loss 0.009694, batch error rate 0.000000%\n",
      "After epoch 89: valid_err_rate: 0.017000% currently going ot do 101 epochs\n",
      "log -44.0\n",
      "At minibatch 44600, batch loss 0.042693, batch error rate 1.000000%\n",
      "At minibatch 44700, batch loss 0.008936, batch error rate 0.000000%\n",
      "At minibatch 44800, batch loss 0.004607, batch error rate 0.000000%\n",
      "At minibatch 44900, batch loss 0.013103, batch error rate 0.000000%\n",
      "At minibatch 45000, batch loss 0.014230, batch error rate 0.000000%\n",
      "After epoch 90: valid_err_rate: 0.017100% currently going ot do 101 epochs\n",
      "log -44.5\n",
      "At minibatch 45100, batch loss 0.013393, batch error rate 0.000000%\n",
      "At minibatch 45200, batch loss 0.009347, batch error rate 0.000000%\n",
      "At minibatch 45300, batch loss 0.003001, batch error rate 0.000000%\n",
      "At minibatch 45400, batch loss 0.004911, batch error rate 0.000000%\n",
      "At minibatch 45500, batch loss 0.023682, batch error rate 1.000000%\n",
      "After epoch 91: valid_err_rate: 0.016700% currently going ot do 101 epochs\n",
      "log -45.0\n",
      "At minibatch 45600, batch loss 0.006755, batch error rate 0.000000%\n",
      "At minibatch 45700, batch loss 0.010401, batch error rate 0.000000%\n",
      "At minibatch 45800, batch loss 0.007973, batch error rate 0.000000%\n",
      "At minibatch 45900, batch loss 0.004516, batch error rate 0.000000%\n",
      "At minibatch 46000, batch loss 0.004364, batch error rate 0.000000%\n",
      "After epoch 92: valid_err_rate: 0.017300% currently going ot do 101 epochs\n",
      "log -45.5\n",
      "At minibatch 46100, batch loss 0.011239, batch error rate 0.000000%\n",
      "At minibatch 46200, batch loss 0.005424, batch error rate 0.000000%\n",
      "At minibatch 46300, batch loss 0.011485, batch error rate 0.000000%\n",
      "At minibatch 46400, batch loss 0.007697, batch error rate 0.000000%\n",
      "At minibatch 46500, batch loss 0.014757, batch error rate 0.000000%\n",
      "After epoch 93: valid_err_rate: 0.016900% currently going ot do 101 epochs\n",
      "log -46.0\n",
      "At minibatch 46600, batch loss 0.007420, batch error rate 0.000000%\n",
      "At minibatch 46700, batch loss 0.007425, batch error rate 0.000000%\n",
      "At minibatch 46800, batch loss 0.007880, batch error rate 0.000000%\n",
      "At minibatch 46900, batch loss 0.009036, batch error rate 0.000000%\n",
      "At minibatch 47000, batch loss 0.004444, batch error rate 0.000000%\n",
      "After epoch 94: valid_err_rate: 0.017300% currently going ot do 101 epochs\n",
      "log -46.5\n",
      "At minibatch 47100, batch loss 0.012393, batch error rate 0.000000%\n",
      "At minibatch 47200, batch loss 0.008819, batch error rate 0.000000%\n",
      "At minibatch 47300, batch loss 0.006176, batch error rate 0.000000%\n",
      "At minibatch 47400, batch loss 0.018765, batch error rate 1.000000%\n",
      "At minibatch 47500, batch loss 0.007939, batch error rate 0.000000%\n",
      "After epoch 95: valid_err_rate: 0.017300% currently going ot do 101 epochs\n",
      "log -47.0\n",
      "At minibatch 47600, batch loss 0.006089, batch error rate 0.000000%\n",
      "At minibatch 47700, batch loss 0.011059, batch error rate 0.000000%\n",
      "At minibatch 47800, batch loss 0.018539, batch error rate 0.000000%\n",
      "At minibatch 47900, batch loss 0.005864, batch error rate 0.000000%\n",
      "At minibatch 48000, batch loss 0.007422, batch error rate 0.000000%\n",
      "After epoch 96: valid_err_rate: 0.017100% currently going ot do 101 epochs\n",
      "log -47.5\n",
      "At minibatch 48100, batch loss 0.009078, batch error rate 0.000000%\n",
      "At minibatch 48200, batch loss 0.005934, batch error rate 0.000000%\n",
      "At minibatch 48300, batch loss 0.012301, batch error rate 0.000000%\n",
      "At minibatch 48400, batch loss 0.007250, batch error rate 0.000000%\n",
      "At minibatch 48500, batch loss 0.006789, batch error rate 0.000000%\n",
      "After epoch 97: valid_err_rate: 0.017100% currently going ot do 101 epochs\n",
      "log -48.0\n",
      "At minibatch 48600, batch loss 0.010966, batch error rate 0.000000%\n",
      "At minibatch 48700, batch loss 0.009606, batch error rate 0.000000%\n",
      "At minibatch 48800, batch loss 0.008283, batch error rate 0.000000%\n",
      "At minibatch 48900, batch loss 0.006770, batch error rate 0.000000%\n",
      "At minibatch 49000, batch loss 0.004869, batch error rate 0.000000%\n",
      "After epoch 98: valid_err_rate: 0.017100% currently going ot do 101 epochs\n",
      "log -48.5\n",
      "At minibatch 49100, batch loss 0.003643, batch error rate 0.000000%\n",
      "At minibatch 49200, batch loss 0.006662, batch error rate 0.000000%\n",
      "At minibatch 49300, batch loss 0.013588, batch error rate 0.000000%\n",
      "At minibatch 49400, batch loss 0.007483, batch error rate 0.000000%\n",
      "At minibatch 49500, batch loss 0.019629, batch error rate 1.000000%\n",
      "After epoch 99: valid_err_rate: 0.017200% currently going ot do 101 epochs\n",
      "log -49.0\n",
      "At minibatch 49600, batch loss 0.004131, batch error rate 0.000000%\n",
      "At minibatch 49700, batch loss 0.003991, batch error rate 0.000000%\n",
      "At minibatch 49800, batch loss 0.003164, batch error rate 0.000000%\n",
      "At minibatch 49900, batch loss 0.010922, batch error rate 0.000000%\n",
      "At minibatch 50000, batch loss 0.010442, batch error rate 0.000000%\n",
      "After epoch 100: valid_err_rate: 0.016700% currently going ot do 101 epochs\n",
      "log -49.5\n",
      "At minibatch 50100, batch loss 0.020400, batch error rate 0.000000%\n",
      "At minibatch 50200, batch loss 0.003676, batch error rate 0.000000%\n",
      "At minibatch 50300, batch loss 0.016797, batch error rate 0.000000%\n",
      "At minibatch 50400, batch loss 0.009244, batch error rate 0.000000%\n",
      "At minibatch 50500, batch loss 0.007013, batch error rate 0.000000%\n",
      "After epoch 101: valid_err_rate: 0.016900% currently going ot do 101 epochs\n",
      "log -50.0\n",
      "At minibatch 50600, batch loss 0.007699, batch error rate 0.000000%\n",
      "At minibatch 50700, batch loss 0.011603, batch error rate 0.000000%\n",
      "At minibatch 50800, batch loss 0.006837, batch error rate 0.000000%\n",
      "At minibatch 50900, batch loss 0.010244, batch error rate 0.000000%\n",
      "At minibatch 51000, batch loss 0.037400, batch error rate 1.000000%\n",
      "After epoch 102: valid_err_rate: 0.017200% currently going ot do 101 epochs\n",
      "Test error rate: 1.770000%\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# TODO: pick a network architecture here. The one below is just \n",
    "# softmax regression\n",
    "#\n",
    "\n",
    "net = FeedForwardNet([\n",
    "        AffineLayer(784,1000),\n",
    "        ReLULayer(),\n",
    "        AffineLayer(1000, 10),\n",
    "        SoftMaxLayer()\n",
    "        ])\n",
    "SGD(net, mnist_train_stream, mnist_validation_stream, mnist_test_stream)\n",
    "\n",
    "print \"Test error rate: %f%%\" % (compute_error_rate(net, mnist_test_stream)*100.0, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0177"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_error_rate(net, mnist_test_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log -inf\n",
      "At minibatch 100, batch loss 0.456175, batch error rate 16.000000%\n",
      "At minibatch 200, batch loss 0.160259, batch error rate 2.000000%\n",
      "At minibatch 300, batch loss 0.242751, batch error rate 9.000000%\n",
      "At minibatch 400, batch loss 0.262258, batch error rate 6.000000%\n",
      "At minibatch 500, batch loss 0.128745, batch error rate 4.000000%\n",
      "After epoch 1: valid_err_rate: 4.480000% currently going ot do 3 epochs\n",
      "log -0.442695040889\n",
      "At minibatch 600, batch loss 0.153069, batch error rate 6.000000%\n",
      "At minibatch 700, batch loss 0.247989, batch error rate 7.000000%\n",
      "At minibatch 800, batch loss 0.107850, batch error rate 4.000000%\n",
      "At minibatch 900, batch loss 0.111959, batch error rate 3.000000%\n",
      "At minibatch 1000, batch loss 0.142231, batch error rate 5.000000%\n",
      "After epoch 2: valid_err_rate: 3.260000% currently going ot do 4 epochs\n",
      "log 0.0897607733732\n",
      "At minibatch 1100, batch loss 0.070440, batch error rate 2.000000%\n",
      "At minibatch 1200, batch loss 0.106972, batch error rate 3.000000%\n",
      "At minibatch 1300, batch loss 0.076764, batch error rate 2.000000%\n",
      "At minibatch 1400, batch loss 0.059965, batch error rate 1.000000%\n",
      "At minibatch 1500, batch loss 0.135664, batch error rate 3.000000%\n",
      "After epoch 3: valid_err_rate: 2.790000% currently going ot do 5 epochs\n",
      "log 0.278652479556\n",
      "At minibatch 1600, batch loss 0.035110, batch error rate 0.000000%\n",
      "At minibatch 1700, batch loss 0.138413, batch error rate 4.000000%\n",
      "At minibatch 1800, batch loss 0.077014, batch error rate 1.000000%\n",
      "At minibatch 1900, batch loss 0.186553, batch error rate 4.000000%\n",
      "At minibatch 2000, batch loss 0.076822, batch error rate 2.000000%\n",
      "After epoch 4: valid_err_rate: 2.780000% currently going ot do 7 epochs\n",
      "log 0.37866506544\n",
      "At minibatch 2100, batch loss 0.064326, batch error rate 3.000000%\n",
      "At minibatch 2200, batch loss 0.105932, batch error rate 4.000000%\n",
      "At minibatch 2300, batch loss 0.046315, batch error rate 1.000000%\n",
      "At minibatch 2400, batch loss 0.059579, batch error rate 3.000000%\n",
      "At minibatch 2500, batch loss 0.068114, batch error rate 3.000000%\n",
      "After epoch 5: valid_err_rate: 2.630000% currently going ot do 8 epochs\n",
      "log 0.441889373449\n",
      "At minibatch 2600, batch loss 0.060015, batch error rate 3.000000%\n",
      "At minibatch 2700, batch loss 0.081562, batch error rate 1.000000%\n",
      "At minibatch 2800, batch loss 0.042775, batch error rate 0.000000%\n",
      "At minibatch 2900, batch loss 0.032275, batch error rate 1.000000%\n",
      "At minibatch 3000, batch loss 0.054716, batch error rate 2.000000%\n",
      "After epoch 6: valid_err_rate: 2.550000% currently going ot do 10 epochs\n",
      "log 0.48610165763\n",
      "At minibatch 3100, batch loss 0.053945, batch error rate 1.000000%\n",
      "At minibatch 3200, batch loss 0.062485, batch error rate 2.000000%\n",
      "At minibatch 3300, batch loss 0.067938, batch error rate 1.000000%\n",
      "At minibatch 3400, batch loss 0.094887, batch error rate 2.000000%\n",
      "At minibatch 3500, batch loss 0.048250, batch error rate 1.000000%\n",
      "After epoch 7: valid_err_rate: 2.420000% currently going ot do 11 epochs\n",
      "log 0.519101653037\n",
      "At minibatch 3600, batch loss 0.141364, batch error rate 3.000000%\n",
      "At minibatch 3700, batch loss 0.081655, batch error rate 3.000000%\n",
      "At minibatch 3800, batch loss 0.043859, batch error rate 1.000000%\n",
      "At minibatch 3900, batch loss 0.096813, batch error rate 3.000000%\n",
      "At minibatch 4000, batch loss 0.092172, batch error rate 2.000000%\n",
      "After epoch 8: valid_err_rate: 2.340000% currently going ot do 13 epochs\n",
      "log 0.544880386687\n",
      "At minibatch 4100, batch loss 0.031446, batch error rate 1.000000%\n",
      "At minibatch 4200, batch loss 0.051301, batch error rate 1.000000%\n",
      "At minibatch 4300, batch loss 0.093421, batch error rate 4.000000%\n",
      "At minibatch 4400, batch loss 0.046789, batch error rate 1.000000%\n",
      "At minibatch 4500, batch loss 0.047870, batch error rate 2.000000%\n",
      "After epoch 9: valid_err_rate: 2.310000% currently going ot do 14 epochs\n",
      "log 0.565705518097\n",
      "At minibatch 4600, batch loss 0.066101, batch error rate 1.000000%\n",
      "At minibatch 4700, batch loss 0.049169, batch error rate 1.000000%\n",
      "At minibatch 4800, batch loss 0.026537, batch error rate 0.000000%\n",
      "At minibatch 4900, batch loss 0.069821, batch error rate 2.000000%\n",
      "At minibatch 5000, batch loss 0.133682, batch error rate 4.000000%\n",
      "After epoch 10: valid_err_rate: 2.310000% currently going ot do 14 epochs\n",
      "log 0.582967608576\n",
      "At minibatch 5100, batch loss 0.011390, batch error rate 0.000000%\n",
      "At minibatch 5200, batch loss 0.054198, batch error rate 1.000000%\n",
      "At minibatch 5300, batch loss 0.045780, batch error rate 1.000000%\n",
      "At minibatch 5400, batch loss 0.039707, batch error rate 0.000000%\n",
      "At minibatch 5500, batch loss 0.065552, batch error rate 2.000000%\n",
      "After epoch 11: valid_err_rate: 2.190000% currently going ot do 17 epochs\n",
      "log 0.597570395618\n",
      "At minibatch 5600, batch loss 0.030361, batch error rate 0.000000%\n",
      "At minibatch 5700, batch loss 0.025504, batch error rate 1.000000%\n",
      "At minibatch 5800, batch loss 0.052490, batch error rate 1.000000%\n",
      "At minibatch 5900, batch loss 0.086225, batch error rate 1.000000%\n",
      "At minibatch 6000, batch loss 0.069100, batch error rate 1.000000%\n",
      "After epoch 12: valid_err_rate: 2.190000% currently going ot do 17 epochs\n",
      "log 0.610128754749\n",
      "At minibatch 6100, batch loss 0.045622, batch error rate 1.000000%\n",
      "At minibatch 6200, batch loss 0.053425, batch error rate 2.000000%\n",
      "At minibatch 6300, batch loss 0.159416, batch error rate 4.000000%\n",
      "At minibatch 6400, batch loss 0.027619, batch error rate 0.000000%\n",
      "At minibatch 6500, batch loss 0.046483, batch error rate 1.000000%\n",
      "After epoch 13: valid_err_rate: 2.240000% currently going ot do 17 epochs\n",
      "log 0.62107681831\n",
      "At minibatch 6600, batch loss 0.075528, batch error rate 1.000000%\n",
      "At minibatch 6700, batch loss 0.032924, batch error rate 1.000000%\n",
      "At minibatch 6800, batch loss 0.045768, batch error rate 1.000000%\n",
      "At minibatch 6900, batch loss 0.041282, batch error rate 1.000000%\n",
      "At minibatch 7000, batch loss 0.040650, batch error rate 1.000000%\n",
      "After epoch 14: valid_err_rate: 2.240000% currently going ot do 17 epochs\n",
      "log 0.630730626931\n",
      "At minibatch 7100, batch loss 0.047176, batch error rate 2.000000%\n",
      "At minibatch 7200, batch loss 0.083383, batch error rate 1.000000%\n",
      "At minibatch 7300, batch loss 0.086394, batch error rate 2.000000%\n",
      "At minibatch 7400, batch loss 0.047957, batch error rate 2.000000%\n",
      "At minibatch 7500, batch loss 0.102235, batch error rate 2.000000%\n",
      "After epoch 15: valid_err_rate: 2.100000% currently going ot do 23 epochs\n",
      "log 0.639326239778\n",
      "At minibatch 7600, batch loss 0.052632, batch error rate 1.000000%\n",
      "At minibatch 7700, batch loss 0.038711, batch error rate 0.000000%\n",
      "At minibatch 7800, batch loss 0.032606, batch error rate 1.000000%\n",
      "At minibatch 7900, batch loss 0.067279, batch error rate 3.000000%\n",
      "At minibatch 8000, batch loss 0.021432, batch error rate 0.000000%\n",
      "After epoch 16: valid_err_rate: 2.190000% currently going ot do 23 epochs\n",
      "log 0.647043876135\n",
      "At minibatch 8100, batch loss 0.032225, batch error rate 0.000000%\n",
      "At minibatch 8200, batch loss 0.053885, batch error rate 2.000000%\n",
      "At minibatch 8300, batch loss 0.032256, batch error rate 1.000000%\n",
      "At minibatch 8400, batch loss 0.060523, batch error rate 2.000000%\n",
      "At minibatch 8500, batch loss 0.024334, batch error rate 0.000000%\n",
      "After epoch 17: valid_err_rate: 2.260000% currently going ot do 23 epochs\n",
      "log 0.654023743739\n",
      "At minibatch 8600, batch loss 0.020678, batch error rate 0.000000%\n",
      "At minibatch 8700, batch loss 0.046146, batch error rate 2.000000%\n",
      "At minibatch 8800, batch loss 0.044271, batch error rate 0.000000%\n",
      "At minibatch 8900, batch loss 0.015432, batch error rate 0.000000%\n",
      "At minibatch 9000, batch loss 0.035497, batch error rate 0.000000%\n",
      "After epoch 18: valid_err_rate: 2.130000% currently going ot do 23 epochs\n",
      "log 0.660376728105\n",
      "At minibatch 9100, batch loss 0.037258, batch error rate 0.000000%\n",
      "At minibatch 9200, batch loss 0.026287, batch error rate 0.000000%\n",
      "At minibatch 9300, batch loss 0.050781, batch error rate 1.000000%\n",
      "At minibatch 9400, batch loss 0.043762, batch error rate 2.000000%\n",
      "At minibatch 9500, batch loss 0.071171, batch error rate 2.000000%\n",
      "After epoch 19: valid_err_rate: 2.110000% currently going ot do 23 epochs\n",
      "log 0.666191799305\n",
      "At minibatch 9600, batch loss 0.026791, batch error rate 0.000000%\n",
      "At minibatch 9700, batch loss 0.022521, batch error rate 0.000000%\n",
      "At minibatch 9800, batch loss 0.037577, batch error rate 0.000000%\n",
      "At minibatch 9900, batch loss 0.050455, batch error rate 2.000000%\n",
      "At minibatch 10000, batch loss 0.028604, batch error rate 1.000000%\n",
      "After epoch 20: valid_err_rate: 2.180000% currently going ot do 23 epochs\n",
      "log 0.671541261247\n",
      "At minibatch 10100, batch loss 0.031102, batch error rate 0.000000%\n",
      "At minibatch 10200, batch loss 0.046467, batch error rate 1.000000%\n",
      "At minibatch 10300, batch loss 0.120886, batch error rate 2.000000%\n",
      "At minibatch 10400, batch loss 0.021955, batch error rate 0.000000%\n",
      "At minibatch 10500, batch loss 0.043744, batch error rate 1.000000%\n",
      "After epoch 21: valid_err_rate: 2.060000% currently going ot do 32 epochs\n",
      "log 0.676484546851\n",
      "At minibatch 10600, batch loss 0.050661, batch error rate 0.000000%\n",
      "At minibatch 10700, batch loss 0.021186, batch error rate 0.000000%\n",
      "At minibatch 10800, batch loss 0.018200, batch error rate 0.000000%\n",
      "At minibatch 10900, batch loss 0.046970, batch error rate 3.000000%\n",
      "At minibatch 11000, batch loss 0.045952, batch error rate 2.000000%\n",
      "After epoch 22: valid_err_rate: 2.050000% currently going ot do 34 epochs\n",
      "log 0.681071011096\n",
      "At minibatch 11100, batch loss 0.042625, batch error rate 0.000000%\n",
      "At minibatch 11200, batch loss 0.031905, batch error rate 0.000000%\n",
      "At minibatch 11300, batch loss 0.025523, batch error rate 0.000000%\n",
      "At minibatch 11400, batch loss 0.073160, batch error rate 3.000000%\n",
      "At minibatch 11500, batch loss 0.066372, batch error rate 1.000000%\n",
      "After epoch 23: valid_err_rate: 2.020000% currently going ot do 35 epochs\n",
      "log 0.685342019556\n",
      "At minibatch 11600, batch loss 0.044759, batch error rate 2.000000%\n",
      "At minibatch 11700, batch loss 0.038326, batch error rate 0.000000%\n",
      "At minibatch 11800, batch loss 0.020642, batch error rate 0.000000%\n",
      "At minibatch 11900, batch loss 0.050089, batch error rate 2.000000%\n",
      "At minibatch 12000, batch loss 0.029314, batch error rate 1.000000%\n",
      "After epoch 24: valid_err_rate: 2.070000% currently going ot do 35 epochs\n",
      "log 0.68933253272\n",
      "At minibatch 12100, batch loss 0.024478, batch error rate 0.000000%\n",
      "At minibatch 12200, batch loss 0.017653, batch error rate 0.000000%\n",
      "At minibatch 12300, batch loss 0.058367, batch error rate 3.000000%\n",
      "At minibatch 12400, batch loss 0.026049, batch error rate 1.000000%\n",
      "At minibatch 12500, batch loss 0.058244, batch error rate 3.000000%\n",
      "After epoch 25: valid_err_rate: 2.040000% currently going ot do 35 epochs\n",
      "log 0.69307232357\n",
      "At minibatch 12600, batch loss 0.032734, batch error rate 0.000000%\n",
      "At minibatch 12700, batch loss 0.035194, batch error rate 1.000000%\n",
      "At minibatch 12800, batch loss 0.056604, batch error rate 1.000000%\n",
      "Setting network parameters from after epoch 23\n",
      "Test error rate: 2.150000%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEDCAYAAADayhiNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3o0pZ2mPqln8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f25e9ce0290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "net = FeedForwardNet([\n",
    "        AffineLayer(784,2500),\n",
    "        ReLULayer(),\n",
    "        AffineLayer(2500, 10),\n",
    "        SoftMaxLayer()\n",
    "        ])\n",
    "SGD(net, mnist_train_stream, mnist_validation_stream, mnist_test_stream)\n",
    "\n",
    "print \"Test error rate: %f%%\" % (compute_error_rate(net, mnist_test_stream)*100.0, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
